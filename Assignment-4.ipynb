{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9bb5135b5015776cf6fc5850611b1f0e",
     "grade": false,
     "grade_id": "cell-f59ba02a0d0bd641",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## FNCE30012: Foundations of Fintech - Assignment 4\n",
    "This assignment builds on Lectures 7 to 9 and on Tutorials 6 and 7. You might want to consider using some of the Python code discussed in those lectures and tutorials to answer some of the questions below.\n",
    "\n",
    "**Important:** It is important that you *do not* change the type (markdwon vs. code) of any cell, *nor* copy/paste/duplicate any cell! If the cell type is markdown, you are supposed to write text, not code, and vice versa. Provide your answer to each question in the allocated cell. *Do not* create additional cells. Answers provided in any other cell will **not** be marked. *Do not* rename the assignment files. All files should be left as is in the assignment directory.\n",
    "\n",
    "### Task\n",
    "You are given two datasets:\n",
    "1. A file called `Assignment4-data.csv`, that contains financial news (headlines) and daily returns for Apple (AAPL). Relying on this dataset, your role as a FinTech student is to explore the relationship between financial news and stock returns.  \n",
    "  \n",
    "  \n",
    "2. A file called `AAPL_returns.csv`, that contains the daily returns for Apple (AAPL).\n",
    "\n",
    "#### Helpful commands\n",
    "You may find the following commands helpful to complete some of the questions.\n",
    "1. How to create a new column using data from existing column? Recall that, in Tutorial 7, we worked with a variable called `FSscore`. Suppose we wanted to divide all the values of this variable by `100` and store the outcome in a new column. This can be done in one step. The code `df['FSscore_scaled'] = df['FSscore']/100` creates a new column with the name `FSscore_scaled` and stores the modified values.\n",
    "  \n",
    " \n",
    "2. How to separate a string variable into a list of strings? The method `split()` splits a string into a list based on a specified separator. The default separator is any white space. However, one can specify the applied separator as an argument. For example, the code `\"a,b,c\".split(\",\")` splits the string `\"a,b,c\"` into the list `[a, b, c]`.  \n",
    "  \n",
    "\n",
    "3. You can use string functions such as `split()` on a `Pandas` dataframe column by using the `str` attribute. For example, `df['alphabets'].str.split(\",\")` returns a series (consider a series as a dataframe with one column) that contains a list obtained by running the split function on each entry in the column named `alphabets`.  \n",
    "  \n",
    "\n",
    "4. How to chain multiple string operations in `Pandas`? Note that a string function on a `Pandas` column returns a series. One can then use another string function on this series to chain multiple operations. For example, the cell below first converts the string to upper case and then calls the split function.  \n",
    "  \n",
    "\n",
    "5. How to combine two or more data frames? For this purpose, one can use the `concat` function from `Pandas`. To combine the dataframes to match indices you can use the `axis=1` argument. Please see https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html for examples.  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b252745e0f9a22d6ca69738c0eebf3a1",
     "grade": false,
     "grade_id": "cell-6f2212a555a80d07",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Please run the following cell to import the required libraries and for string operations example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afca31440bb169e5f651e76e0be4e9a6",
     "grade": false,
     "grade_id": "cell-942156ee2deb1e8a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [A, B, C]\n",
       "1    [D, E, F]\n",
       "2    [A, Z, X]\n",
       "3    [A, S, P]\n",
       "Name: alphabets, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Execute this cell\n",
    "\n",
    "####################### Package Setup ##########################\n",
    "\n",
    "# Disable FutureWarning for better aesthetics.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# essential libraries for this assignment\n",
    "from finml import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# suppress warnings for deprecated methods from TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Example of string operations\n",
    "import pandas as pd\n",
    "example_data = {'alphabets':['a,b,c', 'd,e,f', 'a,z,x', 'a,s,p']} \n",
    "example_df = pd.DataFrame(example_data) \n",
    "\n",
    "# Chain two string operations\n",
    "example_df['alphabets'].str.upper().str.split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ee72d3c9d34c0d6c1fe4cf5695821746",
     "grade": false,
     "grade_id": "cell-3a03cc8def1a7770",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data exploration and transformation\n",
    "The dataset has the following three columns:\n",
    "1. date: This column contains the date of the observation.  \n",
    "  \n",
    "  \n",
    "2. headlines: This column contains the concatenation of headlines for that date. The headlines are separated by the `<end>` string. For example, if there are three headlines `h1`, `h2`, and `h3` on a given day, the headline cell for that day will be the string `h1<end>h2<end>h3`.  \n",
    "  \n",
    "  \n",
    "3. returns: This column contains the daily returns.\n",
    "\n",
    "In your assessment, please address the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bce3584807460fe02c3c3af61853a717",
     "grade": false,
     "grade_id": "cell-567fabf3a843133b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1 (1.5 marks)\n",
    "Load the dataset in a `Pandas` dataframe and write a Python code that plots the time series of the daily Apple returns (returns on the y-axis and dates on the x-axis). Make sure your plot's axes are appropriately labelled.\n",
    "\n",
    "**Note:** Please use `df` as the variable name for the dataframe and the `parse_dates` argument to correctly parse the date column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cdc4b37107eeec8e4c0583df1f79fac3",
     "grade": false,
     "grade_id": "cell-d18d600f2333c8b2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5840320a60b9fe27979bee89b1e0e784",
     "grade": true,
     "grade_id": "cell-1596aee326837de6",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('AAPL_returns.csv', parse_dates=[0]) \n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(x=df['date'], height=df['RET'])\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Daily Returns\")\n",
    "plt.title(\"Apple Returns\")\n",
    "plt.legend(['Returns'], fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cff016563639489261b12368ff3995fe",
     "grade": false,
     "grade_id": "cell-988970cdca1ca8e5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2 (1.5 marks)\n",
    "Write a Python code that plots the time series of daily headline frequencies (the number of headlines per day on the y-axis and the corresponding date on the x-axis). Make sure your plot's axes are appropriately labelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8ae188c55f14e6ed271a0cffb9c8c534",
     "grade": false,
     "grade_id": "cell-d8aca8823cca6a3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "10365f318373a27376498b88183b4933",
     "grade": true,
     "grade_id": "cell-48de7de1402bf96c",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('Assignment4-data.csv', parse_dates=[1]) \n",
    "df['headline_freq'] = df['headlines'].str.split(\"<end>\").str.len()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(x=df['date'], height=df['headline_freq'])\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Headlines\")\n",
    "plt.legend(['Frequency of Headlines'], fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a7db3127b791b3dc9cea9d3d4ab061d2",
     "grade": false,
     "grade_id": "cell-2429642f3fe9eb3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3 (2 marks)\n",
    "We will use neural networks to explore the relationship between the content of financial news and the direction of stock returns, i.e., their classification into positive or negative returns.\n",
    "\n",
    "1. Create a new column called `returns_direction` in the dataframe that classifies daily returns based on their direction: it assigns a given return a value of 1, if the return is positive (i.e, greater than 0), and a value of 0 otherwise. You may find the Numpy function `where()` useful for this question.\n",
    "\n",
    "2. Count the number of days on which the stock had positive and non-positive returns, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9f5f8a98b573844a2cd412d71b2f32d",
     "grade": false,
     "grade_id": "cell-e4552696a02b9b72",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d01081efe2f35245011a98608a829a3a",
     "grade": true,
     "grade_id": "cell-48f32fe35d810de4",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of days with positive days: 1349 \n",
      "The number of days with non-positive days: 1221 \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Assignment4-data.csv')\n",
    "\n",
    "# Creates return_direction column \n",
    "df[\"returns_direction\"] = np.where(df[\"returns\"]>0, 1, 0)\n",
    "\n",
    "# Counts the number of days the stock had positive and non-positive returns\n",
    "num_days_return = df['returns_direction'].value_counts()\n",
    "# 1 means positive\n",
    "# 0 means non-positive\n",
    "print(\"The number of days with positive days: {} \".format(num_days_return[1]))\n",
    "print(\"The number of days with non-positive days: {} \".format(num_days_return[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f24bf915ba157fc8e2d35768b7391996",
     "grade": false,
     "grade_id": "cell-e89736a299f0e319",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4 (3 marks)\n",
    "For this question please restrict your computations to the first 100 headline dates. You can select them by using the `head` function of `Pandas`. Calculate the tf-idf metric for the following word and headline(s) pairs:\n",
    "1. Word \"apple\" in headlines with date 2008-01-07. Store this value in a variable called `aaple_tfidf`.\n",
    "2. Word \"samsung\" in headlines with date 2008-01-17. Store this value in a variable called `samsung_tfidf`.\n",
    "3. Word \"market\" for news headlines with dates 2008-03-06. Store this value in a variable called `market_tfidf`.\n",
    "\n",
    "Please write a Python code that calculates the metrics from the `df` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6113a33eaeab8f443a0a56943615280c",
     "grade": false,
     "grade_id": "cell-3452037452448333",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2b2013306d0415f56ab2111a522903db",
     "grade": true,
     "grade_id": "cell-8434cb12808907a7",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: the following tf-idf scores are with a small amount of data cleaning (removing punctuation from words)\n",
      "The apple tf-idf is: 1.0216512475319814\n",
      "The samsung tf-idf is: 0.0\n",
      "The market tf-idf is: 2.5257286443082556\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "df = pd.read_csv('Assignment4-data.csv')\n",
    "df = df.head(n=100)\n",
    "\n",
    "\n",
    "#a loop that separates (or splits) each word in the document from one another, creating a list of all words\n",
    "raw_document_words = [doc.split() for doc in df['headlines']]\n",
    "\n",
    "document_words = []\n",
    "for word_list in raw_document_words:\n",
    "    temp_list = []\n",
    "    for word in word_list:\n",
    "        temp_list.append(word.translate(str.maketrans('', '', string.punctuation)))\n",
    "    document_words.append(temp_list)\n",
    "    \n",
    "#turn our list into a sorted array (alphabetical order) which is a set (i.e. no duplicate words)\n",
    "vocab = sorted(set(sum(document_words, [])))\n",
    "#create a dictionary that takes each word as a key and their alphabetical order as a value\n",
    "vocab_dict = {k: i + 1 for i, k in enumerate(vocab)}\n",
    "#create a mxn TF matrix and initialise it with zeros. mxn because we have m (100) headlines and n words in our vocab\n",
    "tf = np.zeros((len(df['headlines']), len(vocab)), dtype=int)\n",
    "#for each word in our list of words\n",
    "for i, doc in enumerate(document_words):\n",
    "    for word in doc:\n",
    "        if word == 'apple' or word == 'samsung' or word == 'market':\n",
    "            tf[i, vocab_dict[word] - 1] += 1\n",
    "\n",
    "\n",
    "idf = np.log(tf.shape[0]/tf.astype(bool).sum(axis=0))\n",
    "\n",
    "tf_idf = tf * idf\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i].date == '2008-01-07':\n",
    "        aaple_tfidf = tf_idf[i][vocab_dict['apple'] - 1]\n",
    "    if df.iloc[i].date == '2008-01-17':\n",
    "        samsung_tfidf = tf_idf[i][vocab_dict['samsung'] - 1]\n",
    "    if df.iloc[i].date == '2008-03-06':\n",
    "        market_tfidf = tf_idf[i][vocab_dict['market'] - 1]\n",
    "\n",
    "\n",
    "\n",
    "print(\"NOTE: the following tf-idf scores are with a small amount of data cleaning (removing punctuation from words)\")\n",
    "print(\"The apple tf-idf is: {}\".format(aaple_tfidf))\n",
    "print(\"The samsung tf-idf is: {}\".format(samsung_tfidf))\n",
    "print(\"The market tf-idf is: {}\".format(market_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a85592ef4b1179b674364f341b12f3a6",
     "grade": false,
     "grade_id": "cell-61c8d589a5d409a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5 (3 marks)\n",
    "Build and train a **one**-layer neural network with two units (neurons) to explain return directions based on financial news. Report and interpret the following three performance measures: \"Precision\", \"Recall\", and \"Accuracy\". According to your opinion, which performance measure(s) is (are) most important in the context of linking news headlines to stock returns and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d6018d9470a313f05acc27f2aa9f8a2",
     "grade": false,
     "grade_id": "cell-6dbcd250786a27d0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 5 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7f775f15b5006c654be27585a15528e7",
     "grade": true,
     "grade_id": "cell-bb09a8eadfc0b18a",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 1s - loss: 0.6931 - acc: 0.5097 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6924 - acc: 0.5266 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6917 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6909 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 5/1000\n",
      "1542/1542 - 1s - loss: 0.6901 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 6/1000\n",
      "1542/1542 - 1s - loss: 0.6892 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 7/1000\n",
      "1542/1542 - 1s - loss: 0.6883 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 8/1000\n",
      "1542/1542 - 1s - loss: 0.6874 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 9/1000\n",
      "1542/1542 - 1s - loss: 0.6865 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 10/1000\n",
      "1542/1542 - 1s - loss: 0.6855 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 11/1000\n",
      "1542/1542 - 1s - loss: 0.6846 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 12/1000\n",
      "1542/1542 - 1s - loss: 0.6836 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 13/1000\n",
      "1542/1542 - 1s - loss: 0.6826 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 14/1000\n",
      "1542/1542 - 1s - loss: 0.6817 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 15/1000\n",
      "1542/1542 - 1s - loss: 0.6807 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 16/1000\n",
      "1542/1542 - 1s - loss: 0.6797 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 17/1000\n",
      "1542/1542 - 1s - loss: 0.6787 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 18/1000\n",
      "1542/1542 - 1s - loss: 0.6777 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 19/1000\n",
      "1542/1542 - 1s - loss: 0.6767 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 20/1000\n",
      "1542/1542 - 1s - loss: 0.6757 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 21/1000\n",
      "1542/1542 - 1s - loss: 0.6747 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 22/1000\n",
      "1542/1542 - 1s - loss: 0.6737 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 23/1000\n",
      "1542/1542 - 1s - loss: 0.6727 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 24/1000\n",
      "1542/1542 - 1s - loss: 0.6717 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 25/1000\n",
      "1542/1542 - 1s - loss: 0.6707 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 26/1000\n",
      "1542/1542 - 1s - loss: 0.6697 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 27/1000\n",
      "1542/1542 - 1s - loss: 0.6687 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 28/1000\n",
      "1542/1542 - 1s - loss: 0.6676 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 29/1000\n",
      "1542/1542 - 1s - loss: 0.6666 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 30/1000\n",
      "1542/1542 - 1s - loss: 0.6656 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5223735408560312\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5223735408560312\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.5265888456549935\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5265888456549935\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Assignment4-data.csv')\n",
    "\n",
    "# Creates return_direction column \n",
    "df[\"returns_direction\"] = np.where(df[\"returns\"]>0, 1, 0)\n",
    "\n",
    "X_headlines = df.headlines.values\n",
    "y = df.returns_direction\n",
    "data = split_by_threshold(X_headlines, y, test_size=0.4,)\n",
    "(train_texts, y_train, test_texts, y_test) = data\n",
    "(X_train, X_test, vectorizer, k_best_selector) = ngram_vectorize(data,)\n",
    "\n",
    "\n",
    "data = (X_train, y_train, X_test, y_test)\n",
    "model = build_model(X_train.shape[1:], layers=1, units=2);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef43a24a8f2f747faef42ab2ed3381ba",
     "grade": false,
     "grade_id": "cell-f5d31002b5e9a789",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 5 - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b971104e47c4bff267bed2caba45321",
     "grade": true,
     "grade_id": "cell-2a4a888a1e71ac61",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Precision is for all the times True is classified by the model, how often is it genuinely True. \n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "This is the proportion of all the times the model classified the price going up, how often was it correct.\n",
    "\n",
    "Recall is for all the times the correct classification was True how often did the model classify it correctly.\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "This is the proportion of all the times the price went up how often did the model classify it correctly.\n",
    "\n",
    "Accuracy is how many times the model made a correct classification. \n",
    "Accuracy = (True Positives + True Negatives) / Total\n",
    "This is the proportion of times the model predicted the price going up or not.\n",
    "\n",
    "\n",
    "Since the recall is 1.0 and the precision and accuracy are the same, we can deduce that the model always says that the price is going up. The model is classifying the class which is most common (True, which occurs 52.23% of the time). This is likely due to the one layer, two-unit neural network not understanding the complexity of the dataset and simply classifing all classifications as True. This is a very bad model.\n",
    "\n",
    "\n",
    "Assuming that you are only making buy orders (likely to be the case since the interesting class is stock price going up):   \n",
    "Of these three performance measures, the most important evaluation metric for linking news headlines to stock returns is Precision. This is because the cost of false positives is high. An investor would not want a model that incorrectly classifies the stock price going up when it goes down/doesn't change in reality. If you have a high precision that means when you are predicting positive returns, there is a high probability you are correct. An investor would prefer a model that predicts True with high certainty even though it may mean missing out on potential gains (recall), if the model can guarantee price rises then that is enough.\n",
    "An investor would not want to rely on an algorithm that classifies rises incorrectly.\n",
    "\n",
    "\n",
    "<b>The stock market is a no-called-strike game. You dont have to swing at everything  you can wait for your pitch.\" - Warren Buffet</b>\n",
    "\n",
    "\n",
    "\n",
    "  | Set | Precision  | Recall | Accuracy |\n",
    "| --------- | -----:| -----:| -----:|\n",
    "| Test | 0.5223735408560312 | 1.0 | 0.5223735408560312 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dfac2b98065b891abcd127e360f6d397",
     "grade": false,
     "grade_id": "cell-7f2d6bb9c9a92323",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 6 (3 marks)\n",
    "Explore different neural network models by changing the number of layers and units. You can use up to three layers and five units. \n",
    "\n",
    "Complete the table below by adding your results for the **test** data set. You should duplicate the table format in your own markdown cell and replace the \"-\" placeholders with the corresponding values. Discuss your findings for both the test and train data sets.\n",
    "\n",
    "| Num. Layers  | Num. Units           | Precision  | Recall | Accuracy |\n",
    "| --------- |:---------:| -----:| -----:| -----:|\n",
    "| 1 | 1 | - | - | - |\n",
    "| 2 | 3 | - | - | - |\n",
    "| 3 | 5 | - | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3e767261ae73513cb37e0a7eed1561c9",
     "grade": false,
     "grade_id": "cell-95a1ba66d6df7be1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 6 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "616c0ab8ee97656e0a57ac0479b5a47d",
     "grade": true,
     "grade_id": "cell-5490f30531e4cdab",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 1s - loss: 0.6931 - acc: 0.4987 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6926 - acc: 0.5266 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6920 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6915 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 5/1000\n",
      "1542/1542 - 1s - loss: 0.6909 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 6/1000\n",
      "1542/1542 - 1s - loss: 0.6902 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 7/1000\n",
      "1542/1542 - 1s - loss: 0.6896 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 8/1000\n",
      "1542/1542 - 1s - loss: 0.6889 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 9/1000\n",
      "1542/1542 - 1s - loss: 0.6883 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 10/1000\n",
      "1542/1542 - 1s - loss: 0.6876 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 11/1000\n",
      "1542/1542 - 1s - loss: 0.6869 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 12/1000\n",
      "1542/1542 - 1s - loss: 0.6862 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 13/1000\n",
      "1542/1542 - 1s - loss: 0.6855 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 14/1000\n",
      "1542/1542 - 1s - loss: 0.6849 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 15/1000\n",
      "1542/1542 - 1s - loss: 0.6842 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 16/1000\n",
      "1542/1542 - 1s - loss: 0.6835 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 17/1000\n",
      "1542/1542 - 1s - loss: 0.6828 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 18/1000\n",
      "1542/1542 - 1s - loss: 0.6820 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 19/1000\n",
      "1542/1542 - 1s - loss: 0.6813 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 20/1000\n",
      "1542/1542 - 1s - loss: 0.6806 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 21/1000\n",
      "1542/1542 - 1s - loss: 0.6799 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 22/1000\n",
      "1542/1542 - 1s - loss: 0.6792 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 23/1000\n",
      "1542/1542 - 1s - loss: 0.6785 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 24/1000\n",
      "1542/1542 - 1s - loss: 0.6778 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 25/1000\n",
      "1542/1542 - 1s - loss: 0.6771 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 26/1000\n",
      "1542/1542 - 1s - loss: 0.6764 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 27/1000\n",
      "1542/1542 - 1s - loss: 0.6756 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 28/1000\n",
      "1542/1542 - 1s - loss: 0.6749 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 29/1000\n",
      "1542/1542 - 1s - loss: 0.6742 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 30/1000\n",
      "1542/1542 - 1s - loss: 0.6735 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 31/1000\n",
      "1542/1542 - 1s - loss: 0.6728 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 32/1000\n",
      "1542/1542 - 1s - loss: 0.6720 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 33/1000\n",
      "1542/1542 - 1s - loss: 0.6713 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 34/1000\n",
      "1542/1542 - 1s - loss: 0.6706 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 35/1000\n",
      "1542/1542 - 1s - loss: 0.6699 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 36/1000\n",
      "1542/1542 - 1s - loss: 0.6692 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 37/1000\n",
      "1542/1542 - 1s - loss: 0.6684 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 38/1000\n",
      "1542/1542 - 1s - loss: 0.6677 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 39/1000\n",
      "1542/1542 - 1s - loss: 0.6670 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5223735408560312\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5223735408560312\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.5265888456549935\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5265888456549935\n",
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 1s - loss: 0.6931 - acc: 0.4663 - val_loss: 0.6931 - val_acc: 0.5214\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6931 - acc: 0.5259 - val_loss: 0.6931 - val_acc: 0.5224\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6930 - acc: 0.5266 - val_loss: 0.6931 - val_acc: 0.5224\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6928 - acc: 0.5266 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 5/1000\n",
      "1542/1542 - 1s - loss: 0.6926 - acc: 0.5266 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 6/1000\n",
      "1542/1542 - 1s - loss: 0.6924 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 7/1000\n",
      "1542/1542 - 1s - loss: 0.6922 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 8/1000\n",
      "1542/1542 - 1s - loss: 0.6919 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 9/1000\n",
      "1542/1542 - 1s - loss: 0.6916 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 10/1000\n",
      "1542/1542 - 1s - loss: 0.6914 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 11/1000\n",
      "1542/1542 - 1s - loss: 0.6910 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 12/1000\n",
      "1542/1542 - 1s - loss: 0.6907 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 13/1000\n",
      "1542/1542 - 1s - loss: 0.6904 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 14/1000\n",
      "1542/1542 - 1s - loss: 0.6900 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 15/1000\n",
      "1542/1542 - 1s - loss: 0.6896 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 16/1000\n",
      "1542/1542 - 1s - loss: 0.6892 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 17/1000\n",
      "1542/1542 - 1s - loss: 0.6888 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 18/1000\n",
      "1542/1542 - 1s - loss: 0.6884 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 19/1000\n",
      "1542/1542 - 1s - loss: 0.6879 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 20/1000\n",
      "1542/1542 - 1s - loss: 0.6875 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 21/1000\n",
      "1542/1542 - 1s - loss: 0.6870 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 22/1000\n",
      "1542/1542 - 1s - loss: 0.6866 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 23/1000\n",
      "1542/1542 - 1s - loss: 0.6861 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 24/1000\n",
      "1542/1542 - 1s - loss: 0.6856 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 25/1000\n",
      "1542/1542 - 1s - loss: 0.6851 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 26/1000\n",
      "1542/1542 - 1s - loss: 0.6846 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 27/1000\n",
      "1542/1542 - 1s - loss: 0.6841 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 28/1000\n",
      "1542/1542 - 1s - loss: 0.6835 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 29/1000\n",
      "1542/1542 - 1s - loss: 0.6830 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 30/1000\n",
      "1542/1542 - 1s - loss: 0.6824 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 31/1000\n",
      "1542/1542 - 1s - loss: 0.6819 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 32/1000\n",
      "1542/1542 - 1s - loss: 0.6813 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 33/1000\n",
      "1542/1542 - 1s - loss: 0.6807 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 34/1000\n",
      "1542/1542 - 1s - loss: 0.6802 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 35/1000\n",
      "1542/1542 - 1s - loss: 0.6796 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 36/1000\n",
      "1542/1542 - 1s - loss: 0.6789 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 37/1000\n",
      "1542/1542 - 1s - loss: 0.6783 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 38/1000\n",
      "1542/1542 - 1s - loss: 0.6777 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 39/1000\n",
      "1542/1542 - 1s - loss: 0.6771 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 40/1000\n",
      "1542/1542 - 1s - loss: 0.6764 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 41/1000\n",
      "1542/1542 - 1s - loss: 0.6757 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 42/1000\n",
      "1542/1542 - 1s - loss: 0.6751 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000\n",
      "1542/1542 - 1s - loss: 0.6744 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 44/1000\n",
      "1542/1542 - 1s - loss: 0.6737 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 45/1000\n",
      "1542/1542 - 1s - loss: 0.6730 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 46/1000\n",
      "1542/1542 - 1s - loss: 0.6723 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 47/1000\n",
      "1542/1542 - 1s - loss: 0.6715 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 48/1000\n",
      "1542/1542 - 1s - loss: 0.6708 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 49/1000\n",
      "1542/1542 - 1s - loss: 0.6700 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 50/1000\n",
      "1542/1542 - 1s - loss: 0.6693 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 51/1000\n",
      "1542/1542 - 1s - loss: 0.6685 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 52/1000\n",
      "1542/1542 - 1s - loss: 0.6677 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 53/1000\n",
      "1542/1542 - 1s - loss: 0.6669 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 54/1000\n",
      "1542/1542 - 1s - loss: 0.6660 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 55/1000\n",
      "1542/1542 - 1s - loss: 0.6652 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 56/1000\n",
      "1542/1542 - 1s - loss: 0.6644 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 57/1000\n",
      "1542/1542 - 1s - loss: 0.6635 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 58/1000\n",
      "1542/1542 - 1s - loss: 0.6626 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 59/1000\n",
      "1542/1542 - 1s - loss: 0.6617 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 60/1000\n",
      "1542/1542 - 1s - loss: 0.6608 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5223735408560312\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5223735408560312\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.5265888456549935\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5265888456549935\n",
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 2s - loss: 0.6932 - acc: 0.4734 - val_loss: 0.6932 - val_acc: 0.5039\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6930 - acc: 0.5720 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6927 - acc: 0.6291 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6922 - acc: 0.7114 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 5/1000\n",
      "1542/1542 - 1s - loss: 0.6918 - acc: 0.7704 - val_loss: 0.6931 - val_acc: 0.5010\n",
      "Epoch 6/1000\n",
      "1542/1542 - 1s - loss: 0.6913 - acc: 0.7737 - val_loss: 0.6931 - val_acc: 0.5029\n",
      "Epoch 7/1000\n",
      "1542/1542 - 1s - loss: 0.6908 - acc: 0.7659 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 8/1000\n",
      "1542/1542 - 1s - loss: 0.6902 - acc: 0.7691 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 9/1000\n",
      "1542/1542 - 1s - loss: 0.6896 - acc: 0.7756 - val_loss: 0.6931 - val_acc: 0.5088\n",
      "Epoch 10/1000\n",
      "1542/1542 - 1s - loss: 0.6890 - acc: 0.8016 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 11/1000\n",
      "1542/1542 - 1s - loss: 0.6883 - acc: 0.8210 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 12/1000\n",
      "1542/1542 - 1s - loss: 0.6876 - acc: 0.8366 - val_loss: 0.6930 - val_acc: 0.5088\n",
      "Epoch 13/1000\n",
      "1542/1542 - 1s - loss: 0.6868 - acc: 0.8547 - val_loss: 0.6930 - val_acc: 0.5088\n",
      "Epoch 14/1000\n",
      "1542/1542 - 1s - loss: 0.6860 - acc: 0.8722 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 15/1000\n",
      "1542/1542 - 1s - loss: 0.6852 - acc: 0.8943 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 16/1000\n",
      "1542/1542 - 1s - loss: 0.6843 - acc: 0.9079 - val_loss: 0.6930 - val_acc: 0.5049\n",
      "Epoch 17/1000\n",
      "1542/1542 - 1s - loss: 0.6834 - acc: 0.9183 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 18/1000\n",
      "1542/1542 - 1s - loss: 0.6825 - acc: 0.9313 - val_loss: 0.6929 - val_acc: 0.5126\n",
      "Epoch 19/1000\n",
      "1542/1542 - 1s - loss: 0.6815 - acc: 0.9384 - val_loss: 0.6929 - val_acc: 0.5117\n",
      "Epoch 20/1000\n",
      "1542/1542 - 1s - loss: 0.6805 - acc: 0.9494 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 21/1000\n",
      "1542/1542 - 1s - loss: 0.6794 - acc: 0.9559 - val_loss: 0.6929 - val_acc: 0.5068\n",
      "Epoch 22/1000\n",
      "1542/1542 - 1s - loss: 0.6784 - acc: 0.9598 - val_loss: 0.6929 - val_acc: 0.5107\n",
      "Epoch 23/1000\n",
      "1542/1542 - 1s - loss: 0.6773 - acc: 0.9617 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 24/1000\n",
      "1542/1542 - 1s - loss: 0.6761 - acc: 0.9650 - val_loss: 0.6928 - val_acc: 0.5126\n",
      "Epoch 25/1000\n",
      "1542/1542 - 1s - loss: 0.6749 - acc: 0.9682 - val_loss: 0.6928 - val_acc: 0.5126\n",
      "Epoch 26/1000\n",
      "1542/1542 - 1s - loss: 0.6737 - acc: 0.9702 - val_loss: 0.6928 - val_acc: 0.5146\n",
      "Epoch 27/1000\n",
      "1542/1542 - 1s - loss: 0.6725 - acc: 0.9721 - val_loss: 0.6928 - val_acc: 0.5156\n",
      "Epoch 28/1000\n",
      "1542/1542 - 1s - loss: 0.6712 - acc: 0.9747 - val_loss: 0.6928 - val_acc: 0.5146\n",
      "Epoch 29/1000\n",
      "1542/1542 - 1s - loss: 0.6699 - acc: 0.9780 - val_loss: 0.6928 - val_acc: 0.5117\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5256975036710719\n",
      "Recall:  0.6666666666666666\n",
      "Accuracy Score:  0.5116731517509727\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.966547192353644\n",
      "Recall:  0.9963054187192119\n",
      "Accuracy Score:  0.9798962386511024\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Assignment4-data.csv')\n",
    "\n",
    "# Creates return_direction column \n",
    "df[\"returns_direction\"] = np.where(df[\"returns\"]>0, 1, 0)\n",
    "df.head()\n",
    "X_headlines = df.headlines.values\n",
    "y = df.returns_direction\n",
    "data = split_by_threshold(X_headlines, y, test_size=0.4,)\n",
    "(train_texts, y_train, test_texts, y_test) = data\n",
    "(X_train, X_test, vectorizer, k_best_selector) = ngram_vectorize(data,)\n",
    "data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "model = build_model(X_train.shape[1:], layers=1, units=1);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "\n",
    "model = build_model(X_train.shape[1:], layers=2, units=3);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "\n",
    "model = build_model(X_train.shape[1:], layers=3, units=5);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "37b6d9e8beb5054f36de91f5ee00a41b",
     "grade": false,
     "grade_id": "cell-bce99d4e20f9011e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 6 - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec8fb4cf92349e72056d02a6b2ae497e",
     "grade": true,
     "grade_id": "cell-b4cd52cc02c91ab1",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "| Num. Layers  | Num. Units           | Precision  | Recall | Accuracy |\n",
    "| --------- |:---------:| -----:| -----:| -----:|\n",
    "| 1 | 1 | 0.5223735408560312 | 1.0 | 0.5223735408560312 |\n",
    "| 2 | 3 | 0.5223735408560312 | 1.0 | 0.5223735408560312 |\n",
    "| 3 | 5 | 0.5256975036710719 | 0.6666666666666666 | 0.5116731517509727 |\n",
    "  \n",
    "  \n",
    "The first two neural networks (1/1 and 2/3) the model is again just always guessing True for each instance. This is indicated by the recall being 1.0 and the precision and accuracy being the same. This would be caused by the model not comprehending the complexity of the dataset and just classifying the class which is most common (True, which occurs 52.23% of the time). As the number of layers and units increase to 3 and 5 respectively the models starts classifying instances properly which is reflected in the precision rising and the recall dropping. of the three neural networks tested, 3/5 is the best.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79e091d9cca7e0e310039e9e7f3070d2",
     "grade": false,
     "grade_id": "cell-de71e9154d8be94e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 7 (3 marks)\n",
    "Explore the effects of different splits between the training and testing data on the performance of a given neural network model. \n",
    "\n",
    "Complete the table below by adding your results. You should duplicate the table format in your own markdown cell and replace the \"-\" placeholders with the corresponding values. Discuss your findings.\n",
    "\n",
    "Complete the table below by adding your results for the **test** data set. You should use the same markdown format and simply replace the \"-\" placeholders with the corresponding values. Discuss your findings for the different test and train data sets.\n",
    "\n",
    "| Num. Layers/Num. Units| Train/Test split | Precision  | Recall | Accuracy |\n",
    "| --------- |:---------:| -----:| -----:| -----:|\n",
    "| 2/3 | 90/10 | - |- |- |\n",
    "| 3/5 | 90/10 | - |- |- |\n",
    "| 2/3 | 60/40 | - |- |- |\n",
    "| 3/5 | 60/40 | - |- |- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "023472e2a336588fcf1cb449d3db0b4f",
     "grade": false,
     "grade_id": "cell-fe844a14497f43c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 7 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "21802807215634c37471039f116a8102",
     "grade": true,
     "grade_id": "cell-d1a982406e0230ff",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2313 samples, validate on 257 samples\n",
      "Epoch 1/1000\n",
      "2313/2313 - 2s - loss: 0.6932 - acc: 0.4777 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 2/1000\n",
      "2313/2313 - 1s - loss: 0.6931 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 3/1000\n",
      "2313/2313 - 1s - loss: 0.6930 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 4/1000\n",
      "2313/2313 - 1s - loss: 0.6929 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 5/1000\n",
      "2313/2313 - 1s - loss: 0.6927 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 6/1000\n",
      "2313/2313 - 1s - loss: 0.6925 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 7/1000\n",
      "2313/2313 - 1s - loss: 0.6923 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 8/1000\n",
      "2313/2313 - 1s - loss: 0.6921 - acc: 0.5257 - val_loss: 0.6931 - val_acc: 0.5175\n",
      "Epoch 9/1000\n",
      "2313/2313 - 1s - loss: 0.6918 - acc: 0.5257 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 10/1000\n",
      "2313/2313 - 1s - loss: 0.6916 - acc: 0.5257 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 11/1000\n",
      "2313/2313 - 1s - loss: 0.6913 - acc: 0.5257 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 12/1000\n",
      "2313/2313 - 1s - loss: 0.6909 - acc: 0.5257 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 13/1000\n",
      "2313/2313 - 1s - loss: 0.6906 - acc: 0.5257 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 14/1000\n",
      "2313/2313 - 1s - loss: 0.6902 - acc: 0.5257 - val_loss: 0.6930 - val_acc: 0.5175\n",
      "Epoch 15/1000\n",
      "2313/2313 - 1s - loss: 0.6899 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 16/1000\n",
      "2313/2313 - 1s - loss: 0.6895 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 17/1000\n",
      "2313/2313 - 1s - loss: 0.6891 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 18/1000\n",
      "2313/2313 - 1s - loss: 0.6887 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 19/1000\n",
      "2313/2313 - 1s - loss: 0.6883 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 20/1000\n",
      "2313/2313 - 1s - loss: 0.6879 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 21/1000\n",
      "2313/2313 - 1s - loss: 0.6875 - acc: 0.5257 - val_loss: 0.6929 - val_acc: 0.5175\n",
      "Epoch 22/1000\n",
      "2313/2313 - 1s - loss: 0.6870 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 23/1000\n",
      "2313/2313 - 1s - loss: 0.6866 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 24/1000\n",
      "2313/2313 - 1s - loss: 0.6861 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 25/1000\n",
      "2313/2313 - 1s - loss: 0.6857 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 26/1000\n",
      "2313/2313 - 1s - loss: 0.6852 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 27/1000\n",
      "2313/2313 - 1s - loss: 0.6847 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 28/1000\n",
      "2313/2313 - 1s - loss: 0.6843 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 29/1000\n",
      "2313/2313 - 1s - loss: 0.6838 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 30/1000\n",
      "2313/2313 - 1s - loss: 0.6833 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 31/1000\n",
      "2313/2313 - 1s - loss: 0.6828 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 32/1000\n",
      "2313/2313 - 1s - loss: 0.6822 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 33/1000\n",
      "2313/2313 - 1s - loss: 0.6817 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 34/1000\n",
      "2313/2313 - 1s - loss: 0.6812 - acc: 0.5257 - val_loss: 0.6928 - val_acc: 0.5175\n",
      "Epoch 35/1000\n",
      "2313/2313 - 1s - loss: 0.6806 - acc: 0.5257 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 36/1000\n",
      "2313/2313 - 1s - loss: 0.6801 - acc: 0.5257 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 37/1000\n",
      "2313/2313 - 1s - loss: 0.6795 - acc: 0.5257 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 38/1000\n",
      "2313/2313 - 1s - loss: 0.6789 - acc: 0.5257 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Epoch 39/1000\n",
      "2313/2313 - 1s - loss: 0.6784 - acc: 0.5257 - val_loss: 0.6927 - val_acc: 0.5175\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5175097276264592\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5175097276264592\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.5257241677475141\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5257241677475141\n",
      "Train on 2313 samples, validate on 257 samples\n",
      "Epoch 1/1000\n",
      "2313/2313 - 1s - loss: 0.6932 - acc: 0.4743 - val_loss: 0.6932 - val_acc: 0.4942\n",
      "Epoch 2/1000\n",
      "2313/2313 - 1s - loss: 0.6930 - acc: 0.5819 - val_loss: 0.6932 - val_acc: 0.5019\n",
      "Epoch 3/1000\n",
      "2313/2313 - 1s - loss: 0.6927 - acc: 0.6412 - val_loss: 0.6932 - val_acc: 0.4669\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.4897959183673469\n",
      "Recall:  0.7218045112781954\n",
      "Accuracy Score:  0.4669260700389105\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.6714780600461894\n",
      "Recall:  0.9564144736842105\n",
      "Accuracy Score:  0.7310851707738867\n",
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 1s - loss: 0.6931 - acc: 0.4663 - val_loss: 0.6931 - val_acc: 0.5214\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6931 - acc: 0.5259 - val_loss: 0.6931 - val_acc: 0.5224\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6930 - acc: 0.5266 - val_loss: 0.6931 - val_acc: 0.5224\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6928 - acc: 0.5266 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 5/1000\n",
      "1542/1542 - 1s - loss: 0.6926 - acc: 0.5266 - val_loss: 0.6930 - val_acc: 0.5224\n",
      "Epoch 6/1000\n",
      "1542/1542 - 1s - loss: 0.6924 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 7/1000\n",
      "1542/1542 - 1s - loss: 0.6922 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 8/1000\n",
      "1542/1542 - 1s - loss: 0.6919 - acc: 0.5266 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 9/1000\n",
      "1542/1542 - 1s - loss: 0.6916 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 10/1000\n",
      "1542/1542 - 1s - loss: 0.6914 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 11/1000\n",
      "1542/1542 - 1s - loss: 0.6910 - acc: 0.5266 - val_loss: 0.6928 - val_acc: 0.5224\n",
      "Epoch 12/1000\n",
      "1542/1542 - 1s - loss: 0.6907 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 13/1000\n",
      "1542/1542 - 1s - loss: 0.6904 - acc: 0.5266 - val_loss: 0.6927 - val_acc: 0.5224\n",
      "Epoch 14/1000\n",
      "1542/1542 - 1s - loss: 0.6900 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 15/1000\n",
      "1542/1542 - 1s - loss: 0.6896 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 16/1000\n",
      "1542/1542 - 1s - loss: 0.6892 - acc: 0.5266 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 17/1000\n",
      "1542/1542 - 1s - loss: 0.6888 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 18/1000\n",
      "1542/1542 - 1s - loss: 0.6884 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 19/1000\n",
      "1542/1542 - 1s - loss: 0.6879 - acc: 0.5266 - val_loss: 0.6925 - val_acc: 0.5224\n",
      "Epoch 20/1000\n",
      "1542/1542 - 1s - loss: 0.6875 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 21/1000\n",
      "1542/1542 - 1s - loss: 0.6870 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 22/1000\n",
      "1542/1542 - 1s - loss: 0.6866 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 23/1000\n",
      "1542/1542 - 1s - loss: 0.6861 - acc: 0.5266 - val_loss: 0.6924 - val_acc: 0.5224\n",
      "Epoch 24/1000\n",
      "1542/1542 - 1s - loss: 0.6856 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 25/1000\n",
      "1542/1542 - 1s - loss: 0.6851 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 26/1000\n",
      "1542/1542 - 1s - loss: 0.6846 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 27/1000\n",
      "1542/1542 - 1s - loss: 0.6841 - acc: 0.5266 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 28/1000\n",
      "1542/1542 - 1s - loss: 0.6835 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 29/1000\n",
      "1542/1542 - 1s - loss: 0.6830 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 30/1000\n",
      "1542/1542 - 1s - loss: 0.6824 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 31/1000\n",
      "1542/1542 - 1s - loss: 0.6819 - acc: 0.5266 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 32/1000\n",
      "1542/1542 - 1s - loss: 0.6813 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 33/1000\n",
      "1542/1542 - 1s - loss: 0.6807 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 34/1000\n",
      "1542/1542 - 1s - loss: 0.6802 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000\n",
      "1542/1542 - 1s - loss: 0.6796 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 36/1000\n",
      "1542/1542 - 1s - loss: 0.6789 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 37/1000\n",
      "1542/1542 - 1s - loss: 0.6783 - acc: 0.5266 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 38/1000\n",
      "1542/1542 - 1s - loss: 0.6777 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 39/1000\n",
      "1542/1542 - 1s - loss: 0.6771 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 40/1000\n",
      "1542/1542 - 1s - loss: 0.6764 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 41/1000\n",
      "1542/1542 - 1s - loss: 0.6757 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 42/1000\n",
      "1542/1542 - 1s - loss: 0.6751 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 43/1000\n",
      "1542/1542 - 1s - loss: 0.6744 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 44/1000\n",
      "1542/1542 - 1s - loss: 0.6737 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 45/1000\n",
      "1542/1542 - 1s - loss: 0.6730 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 46/1000\n",
      "1542/1542 - 1s - loss: 0.6723 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 47/1000\n",
      "1542/1542 - 1s - loss: 0.6715 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 48/1000\n",
      "1542/1542 - 1s - loss: 0.6708 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 49/1000\n",
      "1542/1542 - 1s - loss: 0.6700 - acc: 0.5266 - val_loss: 0.6920 - val_acc: 0.5224\n",
      "Epoch 50/1000\n",
      "1542/1542 - 1s - loss: 0.6693 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 51/1000\n",
      "1542/1542 - 1s - loss: 0.6685 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 52/1000\n",
      "1542/1542 - 1s - loss: 0.6677 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 53/1000\n",
      "1542/1542 - 1s - loss: 0.6669 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 54/1000\n",
      "1542/1542 - 1s - loss: 0.6660 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 55/1000\n",
      "1542/1542 - 1s - loss: 0.6652 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 56/1000\n",
      "1542/1542 - 1s - loss: 0.6644 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 57/1000\n",
      "1542/1542 - 1s - loss: 0.6635 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 58/1000\n",
      "1542/1542 - 1s - loss: 0.6626 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 59/1000\n",
      "1542/1542 - 1s - loss: 0.6617 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Epoch 60/1000\n",
      "1542/1542 - 1s - loss: 0.6608 - acc: 0.5266 - val_loss: 0.6919 - val_acc: 0.5224\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5223735408560312\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5223735408560312\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.5265888456549935\n",
      "Recall:  1.0\n",
      "Accuracy Score:  0.5265888456549935\n",
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 1s - loss: 0.6932 - acc: 0.4734 - val_loss: 0.6932 - val_acc: 0.5039\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6930 - acc: 0.5720 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6927 - acc: 0.6291 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6922 - acc: 0.7114 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 5/1000\n",
      "1542/1542 - 1s - loss: 0.6918 - acc: 0.7704 - val_loss: 0.6931 - val_acc: 0.5010\n",
      "Epoch 6/1000\n",
      "1542/1542 - 1s - loss: 0.6913 - acc: 0.7737 - val_loss: 0.6931 - val_acc: 0.5029\n",
      "Epoch 7/1000\n",
      "1542/1542 - 1s - loss: 0.6908 - acc: 0.7659 - val_loss: 0.6931 - val_acc: 0.5019\n",
      "Epoch 8/1000\n",
      "1542/1542 - 1s - loss: 0.6902 - acc: 0.7691 - val_loss: 0.6931 - val_acc: 0.5058\n",
      "Epoch 9/1000\n",
      "1542/1542 - 1s - loss: 0.6896 - acc: 0.7756 - val_loss: 0.6931 - val_acc: 0.5088\n",
      "Epoch 10/1000\n",
      "1542/1542 - 1s - loss: 0.6890 - acc: 0.8016 - val_loss: 0.6930 - val_acc: 0.5097\n",
      "Epoch 11/1000\n",
      "1542/1542 - 1s - loss: 0.6883 - acc: 0.8210 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 12/1000\n",
      "1542/1542 - 1s - loss: 0.6876 - acc: 0.8366 - val_loss: 0.6930 - val_acc: 0.5088\n",
      "Epoch 13/1000\n",
      "1542/1542 - 1s - loss: 0.6868 - acc: 0.8547 - val_loss: 0.6930 - val_acc: 0.5088\n",
      "Epoch 14/1000\n",
      "1542/1542 - 1s - loss: 0.6860 - acc: 0.8722 - val_loss: 0.6930 - val_acc: 0.5078\n",
      "Epoch 15/1000\n",
      "1542/1542 - 1s - loss: 0.6852 - acc: 0.8943 - val_loss: 0.6930 - val_acc: 0.5117\n",
      "Epoch 16/1000\n",
      "1542/1542 - 1s - loss: 0.6843 - acc: 0.9079 - val_loss: 0.6930 - val_acc: 0.5049\n",
      "Epoch 17/1000\n",
      "1542/1542 - 1s - loss: 0.6834 - acc: 0.9183 - val_loss: 0.6929 - val_acc: 0.5097\n",
      "Epoch 18/1000\n",
      "1542/1542 - 1s - loss: 0.6825 - acc: 0.9313 - val_loss: 0.6929 - val_acc: 0.5126\n",
      "Epoch 19/1000\n",
      "1542/1542 - 1s - loss: 0.6815 - acc: 0.9384 - val_loss: 0.6929 - val_acc: 0.5117\n",
      "Epoch 20/1000\n",
      "1542/1542 - 1s - loss: 0.6805 - acc: 0.9494 - val_loss: 0.6929 - val_acc: 0.5078\n",
      "Epoch 21/1000\n",
      "1542/1542 - 1s - loss: 0.6794 - acc: 0.9559 - val_loss: 0.6929 - val_acc: 0.5068\n",
      "Epoch 22/1000\n",
      "1542/1542 - 1s - loss: 0.6784 - acc: 0.9598 - val_loss: 0.6929 - val_acc: 0.5107\n",
      "Epoch 23/1000\n",
      "1542/1542 - 1s - loss: 0.6773 - acc: 0.9617 - val_loss: 0.6929 - val_acc: 0.5136\n",
      "Epoch 24/1000\n",
      "1542/1542 - 1s - loss: 0.6761 - acc: 0.9650 - val_loss: 0.6928 - val_acc: 0.5126\n",
      "Epoch 25/1000\n",
      "1542/1542 - 1s - loss: 0.6749 - acc: 0.9682 - val_loss: 0.6928 - val_acc: 0.5126\n",
      "Epoch 26/1000\n",
      "1542/1542 - 1s - loss: 0.6737 - acc: 0.9702 - val_loss: 0.6928 - val_acc: 0.5146\n",
      "Epoch 27/1000\n",
      "1542/1542 - 1s - loss: 0.6725 - acc: 0.9721 - val_loss: 0.6928 - val_acc: 0.5156\n",
      "Epoch 28/1000\n",
      "1542/1542 - 1s - loss: 0.6712 - acc: 0.9747 - val_loss: 0.6928 - val_acc: 0.5146\n",
      "Epoch 29/1000\n",
      "1542/1542 - 1s - loss: 0.6699 - acc: 0.9780 - val_loss: 0.6928 - val_acc: 0.5117\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.5256975036710719\n",
      "Recall:  0.6666666666666666\n",
      "Accuracy Score:  0.5116731517509727\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.966547192353644\n",
      "Recall:  0.9963054187192119\n",
      "Accuracy Score:  0.9798962386511024\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Assignment4-data.csv')\n",
    "\n",
    "# Creates return_direction column \n",
    "df[\"returns_direction\"] = np.where(df[\"returns\"]>0, 1, 0)\n",
    "\n",
    "X_headlines = df.headlines.values\n",
    "y = df.returns_direction\n",
    "data = split_by_threshold(X_headlines, y, test_size=0.1,)\n",
    "(train_texts, y_train, test_texts, y_test) = data\n",
    "(X_train, X_test, vectorizer, k_best_selector) = ngram_vectorize(data,)\n",
    "data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "model = build_model(X_train.shape[1:], layers=2, units=3);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "model = build_model(X_train.shape[1:], layers=3, units=5);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "\n",
    "\n",
    "data = split_by_threshold(X_headlines, y, test_size=0.4,)\n",
    "(train_texts, y_train, test_texts, y_test) = data\n",
    "(X_train, X_test, vectorizer, k_best_selector) = ngram_vectorize(data,)\n",
    "data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "model = build_model(X_train.shape[1:], layers=2, units=3);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "model = build_model(X_train.shape[1:], layers=3, units=5);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8116753c79645e5b6e9f310a96adc5a7",
     "grade": false,
     "grade_id": "cell-08557321ae89e903",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 7 - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a07ce2cc44e5122eaf7f3ce2026ffd4f",
     "grade": true,
     "grade_id": "cell-940c3b0183e89cb2",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "| Num. Layers/Num. Units| Train/Test split | Precision  | Recall | Accuracy |\n",
    "| --------- |:---------:| -----:| -----:| -----:|\n",
    "| 2/3 | 90/10 | 0.5175097276264592 | 1.0 | 0.5175097276264592 |\n",
    "| 3/5 | 90/10 | 0.4897959183673469 | 0.7218045112781954 | 0.4669260700389105 |\n",
    "| 2/3 | 60/40 | 0.5223735408560312 | 1.0 | 0.5223735408560312 |\n",
    "| 3/5 | 60/40 | 0.5256975036710719 | 0.6666666666666666 | 0.5116731517509727 |\n",
    "\n",
    "\n",
    "As the Train/Test split goes from 90/10 to 60/40 the precision improves and the recall drops. This is most likely because in the 90 Train/10 Test Splits, the model is being overfitted since the model performs much better in the training set compared to the test set. This means the model is built too closely to the training set and has picked up the nuances of the dataset which is most likely just noise. The 60/40 does a better job of generalising the model since it isn't giving the model too much data for the model to fit to. Although it is typically thought that you want to give as much Testing and Training data as possible there are a lot of trade-offs and 60/40 seems to work well.  \n",
    "\n",
    "With the 3/5 networks, the recall falls from 0.722 to 0.667 as the model is getting better classifying which is also seen in the precison and accuracy too. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "94313898db78e5f939701a282521b223",
     "grade": false,
     "grade_id": "cell-e0e8f3ca9b702a7a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 8 (3 marks)\n",
    "Run a logistic regression with the same independent and dependent variables as used for the above neural network models. You have access to the `sklearn` package, which should help you answering this question. To work with the `sklearn` package, you may find the following links helpful.\n",
    "1. Building a logit model: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "  \n",
    "  \n",
    "2. Evaluating a logit model: \n",
    "    - Recall: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "    - Precision: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "    - Accuracy: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "    \n",
    "Compare and contrast your findings with the above findings based on neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "30ec1bc06f533b28d0e793478bcdf318",
     "grade": false,
     "grade_id": "cell-c8dcec9bd6416eb6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 8 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d2cb3558ca4d6ff52186add573dd93a0",
     "grade": true,
     "grade_id": "cell-d6c261e8822ba347",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is: 0.5227606461086637\n",
      "Recall is: 0.6629422718808193\n",
      "Accuracy is: 0.5077821011673151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('Assignment4-data.csv')\n",
    "\n",
    "# Creates return_direction column \n",
    "df[\"returns_direction\"] = np.where(df[\"returns\"]>0, 1, 0)\n",
    "\n",
    "X_headlines = df.headlines.values\n",
    "y = df.returns_direction\n",
    "data = split_by_threshold(X_headlines, y, test_size=0.4,)\n",
    "(train_texts, y_train, test_texts, y_test) = data\n",
    "(X_train, X_test, vectorizer, k_best_selector) = ngram_vectorize(data,)\n",
    "data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
    "y_prediction = clf.predict(X_test)\n",
    "\n",
    "recall = recall_score(y_test, y_prediction)\n",
    "precision = precision_score(y_test, y_prediction)\n",
    "accuracy = accuracy_score(y_test, y_prediction)\n",
    "\n",
    "print(\"Precision is: {}\".format(precision))\n",
    "print(\"Recall is: {}\".format(recall))\n",
    "print(\"Accuracy is: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa49d79d1dcad63f35aa2f6a1af2b467",
     "grade": false,
     "grade_id": "cell-0826e66e97970a3c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 8 - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dee0f19205aa66e99fb562da206c306a",
     "grade": true,
     "grade_id": "cell-ecda2393f9a4bac3",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The Logistic Regression is a decent classifier. It is better than some of the lower order neural networks (1/2, 2/3) as they just classified True for all instances whereas the logistic regression is genuinely starting to understanding the dataset and makes classifications. The logistic regression is limited by the fact that it has a single sigmoid function so can not truly understand the complexities of the natural language like a multi-layer/unit neural network can. \n",
    "\n",
    "A pro of the Logistic regression is that it does not need to be trained the way that a neural network does so can output results much faster.\n",
    "\n",
    "Of the neural networks tested, the 3 layers/5 units with 60 training/40 testing performed the best because it was able to get the closest to understanding the complexity of the data and although it took longer than the Logistic Regression, it was able to perform slightly better so it is the preferred classifier to use.\n",
    "\n",
    "\n",
    "\n",
    "| Classifiers | Precision  | Recall | Accuracy |\n",
    "| --------- | -----:| -----:| -----:|\n",
    "| Best Neural Network Tested (3/5, 60/40) | 0.5256975036710719 | 0.6666666666666666 | 0.5116731517509727 |\n",
    "| Logistic Regression | 0.5227606461086637 | 0.6629422718808193 | 0.5077821011673151 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f72fa4595755fa6700fe9517f104ab9",
     "grade": false,
     "grade_id": "cell-693d8ece9b0c38e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 9 (5 marks)\n",
    "Everything you did so far was explaining stock returns with contemporaneous financial news that were released on the same date. To explore how well a neural network can **predict** the direction of **future** returns based on our text data, you should do the following.\n",
    "1. Please read the `AAPL_returns.csv` into a dataframe by using the `parse_dates` argument and create a new column  `returns_pred` by shifting the returns by one **trading** day. For this purpose, you may find the `shift` function from `Pandas` helpful.\n",
    "  \n",
    "  \n",
    "2. Combine the `df` dataframe that contains headlines with this new dataframe such that for a given headline date, the value in `returns_pred` contains the return on the **subsequent** trading day.\n",
    "  \n",
    "  \n",
    "3. Train a neural network that uses financial news to learn the `returns_pred` variable. You are allowed to use any of the above neural network parameterisations and train/test data splits.\n",
    "  \n",
    "  \n",
    "4. Explain your findings with regard to the given data and your chosen parameters. Interpret your results in the context of the Efficient Market Hypothesis (EMH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5ff4792a4f889a4c60692cbe016d7969",
     "grade": false,
     "grade_id": "cell-0edf71034ed5dcaf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 9 - Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0ea0ac43f6e14f3acc1e6cb9cf5e4726",
     "grade": true,
     "grade_id": "cell-08818ac96ab94352",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1542 samples, validate on 1028 samples\n",
      "Epoch 1/1000\n",
      "1542/1542 - 1s - loss: 0.6932 - acc: 0.4728 - val_loss: 0.6931 - val_acc: 0.5340\n",
      "Epoch 2/1000\n",
      "1542/1542 - 1s - loss: 0.6929 - acc: 0.6278 - val_loss: 0.6931 - val_acc: 0.5321\n",
      "Epoch 3/1000\n",
      "1542/1542 - 1s - loss: 0.6924 - acc: 0.7393 - val_loss: 0.6931 - val_acc: 0.5272\n",
      "Epoch 4/1000\n",
      "1542/1542 - 1s - loss: 0.6919 - acc: 0.8003 - val_loss: 0.6931 - val_acc: 0.5282\n",
      "Training Completed\n",
      "########################################\n",
      "############## Test Set: ###############\n",
      "########################################\n",
      "Precision:  0.534850640113798\n",
      "Recall:  0.704119850187266\n",
      "Accuracy Score:  0.5282101167315175\n",
      "\n",
      "########################################\n",
      "############## Train Set: ##############\n",
      "########################################\n",
      "Precision:  0.7848360655737705\n",
      "Recall:  0.942189421894219\n",
      "Accuracy Score:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('AAPL_returns.csv', parse_dates=['date']) \n",
    "\n",
    "df1['returns_pred'] = df1['RET'].shift(-1, fill_value=0)\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('Assignment4-data.csv', parse_dates=['date']) \n",
    "\n",
    "df = pd.merge(df1, df2, on='date')\n",
    "\n",
    "# Creates return_direction column \n",
    "df[\"returns_direction\"] = np.where(df[\"returns_pred\"]>0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "X_headlines = df.headlines.values\n",
    "y = df.returns_direction\n",
    "data = split_by_threshold(X_headlines, y, test_size=0.4,)\n",
    "(train_texts, y_train, test_texts, y_test) = data\n",
    "(X_train, X_test, vectorizer, k_best_selector) = ngram_vectorize(data,)\n",
    "data = (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "model = build_model(X_train.shape[1:], layers=3, units=5);\n",
    "model = train_model(data, model)\n",
    "evaluate(model, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f1798458af3e9329943c51de247def22",
     "grade": false,
     "grade_id": "cell-3f3937ac783d676d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Answer 9 - Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d98871bd7611022966b57985cf681cf7",
     "grade": true,
     "grade_id": "cell-af1b55d69e9c64e7",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The Classifier I decided to use was the neural network with 3 Layers and 5 Units. This is because of all the classifiers tested, that performed the best.\n",
    "\n",
    "Interestingly the Classifier is better at predicting the next day return direction compared to the current day direction for apple stocks. This may be because some headlines may be released after the market closes so are not incorporated into the price of the current day. \n",
    "\n",
    "\n",
    "(Assuming that headlines encapsulate all public data)\n",
    "Overall, our classifier disproves the efficient market hypothesis since the public information (headlines) is not being factored into the price on the day it happened. The model could not accurately predict the direction of the apple shares even though it had the headlines and processed the language. If the efficient market hypothesis were true the would react the moment the headline hit and we would be able to predict the price changes (which is not the case). This is an invalid assumption since there is more public data that may not make it to a headline but is necessary to answer the question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Returns Date | Precision  | Recall | Accuracy |\n",
    "| --------- | -----:| -----:| -----:|\n",
    "| Same Day | 0.5256975036710719 | 0.6666666666666666 | 0.5116731517509727 |\n",
    "| Next Day | 0.534850640113798 | 0.704119850187266 | 0.5282101167315175 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
